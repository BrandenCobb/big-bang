# Define global rules for when pipelines run
workflow:
  rules:
    # TODO Add docs back in as an exclusion
    # always skip for hotfixes and patches
    - if: '$CI_MERGE_REQUEST_SOURCE_BRANCH_NAME =~ /^(hotfix|patch)/'
      when: never
    # run pipelines for merge request events
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event"'

include:
  - project: 'platform-one/big-bang/pipeline-templates/umbrella-templates'
    file: '/global.gitlab-ci.yml'

stages:
  - smoke tests
  - bundle
  - network up
  - cluster up
  - bigbang up
  - test
  - bigbang down
  - cluster down
  - network down

#-----------------------------------------------------------------------------------------------------------------------
# Pre Stage Jobs
#

pre vars:
  image: registry.dsop.io/platform-one/big-bang/pipeline-templates/pipeline-templates/pre-envs:ubi8.3
  stage: .pre
  artifacts:
    reports:
      dotenv: variables.env
  script:
  # obtain MR and master versions
  - CHART_MR_VERSION=$(sed -n -e 's/^version. //p' chart/Chart.yaml)
  - git fetch && git checkout ${CI_DEFAULT_BRANCH}
  - CHART_MA_VERSION=$(sed -n -e 's/^version. //p' chart/Chart.yaml)
  - git fetch && git checkout ${CI_COMMIT_REF_NAME}
  - echo "CHART_MR_VERSION=$CHART_MR_VERSION" >> variables.env
  - echo "CHART_MA_VERSION=$CHART_MA_VERSION" >> variables.env
  # obtain semver differences (subtract master version from mr verison)
  - chmod +x ./scripts/semver_diff.sh
  - CHART_VERSION_DIFF=$(./scripts/semver_diff.sh $CHART_MR_VERSION $CHART_MA_VERSION)
  - IFS=. DIFF_ARR=(${CHART_VERSION_DIFF##*-})
  - echo "CHART_VERSION_DIFF=$CHART_VERSION_DIFF" >> variables.env
  # detect breaking change (first two version sections in semver diff)
  - CHART_BREAKING_CHANGE="false"
  - if (( ${DIFF_ARR[0]} > 0 )); then CHART_BREAKING_CHANGE="true"; fi
  - if (( ${DIFF_ARR[1]} > 0 )); then CHART_BREAKING_CHANGE="true"; fi
  # store variables
  - echo "CHART_BREAKING_CHANGE=$CHART_BREAKING_CHANGE" >> variables.env
  - cat variables.env

#-----------------------------------------------------------------------------------------------------------------------

#-----------------------------------------------------------------------------------------------------------------------
# Smoke Tests
#

default:
  image: registry.dsop.io/platform-one/big-bang/pipeline-templates/pipeline-templates/k3d-builder:0.0.1

.deploy_bigbang: &deploy_bigbang
  - for script in ./scripts/deploy/*.sh; do bash $script; done

.test_bigbang: &test_bigbang
  - for test in ./tests/bash/*.sh; do bash $test; done 

clean install:
  stage: smoke tests
  extends:
    - .k3d
  rules:
    # Skip on merge request events
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event"'
      when: never
    # Run for branch pipelines
    - if: '$CI_COMMIT_BRANCH'
  variables:
    CLUSTER_NAME: "clean-${CI_COMMIT_SHORT_SHA}"
  script:
    - *deploy_bigbang
    - *test_bigbang

upgrade:
  stage: smoke tests
  needs:
    - job: pre vars
      artifacts: true  
  extends:
    - .k3d
  rules:
    - if: "$CI_MERGE_REQUEST_TITLE =~ /^Breaking Change/'"
      when: never
  variables:
    CLUSTER_NAME: "clean-${CI_COMMIT_SHORT_SHA}"
  script:
    - if $CHART_BREAKING_CHANGE; then echo "Breaking change detected by chart version difference, skipping job"; exit 0; fi
    - echo "Install Big Bang from ${CI_DEFAULT_BRANCH}"
    - git fetch && git checkout ${CI_DEFAULT_BRANCH}
    - *deploy_bigbang
    - *test_bigbang
    - echo "Upgrade Big Bang from ${CI_MERGE_REQUEST_SOURCE_BRANCH_NAME}"
    - git reset --hard && git clean -fd
    - git checkout ${CI_MERGE_REQUEST_SOURCE_BRANCH_NAME}
    - *deploy_bigbang
    - *test_bigbang

#-----------------------------------------------------------------------------------------------------------------------

#-----------------------------------------------------------------------------------------------------------------------
# Bundling
#

# bundle repos
bundle repos:
  image: registry.dsop.io/platform-one/big-bang/pipeline-templates/pipeline-templates/python:3
  stage: bundle
  artifacts:
    paths:
      - airgap/tarballs/repos.tar.gz
  script:
  # bundle umbrella repository
  - ./scripts/bundle_umbrella.sh airgap/repos/umbrella
  # bundle repositories from chart/values.yaml
  - pip install $(./scripts/bundle_repos.py pip)
  - ./scripts/bundle_repos.py bundle chart/values.yaml airgap/repos/packages
  # create repos tarball
  - cd airgap/repos
  - find . -type f -name '*.bundle'
  - tar -zcvf $CI_PROJECT_DIR/airgap/tarballs/repos.tar.gz . >/dev/null

# generate images
# TODO - we need to bump the gitlab artifact max size / generally tidy up gitlab runners
bundle images:
  stage: bundle
  extends:
    - .k3d
  artifacts:
    paths:
      - airgap/tarballs/images.tar.gz
  script:
  # obtain false images from blank cluster state
  - ./scripts/list_images.sh > airgap/lists/images-false.txt
  - cat airgap/lists/images-false.txt
  # deploy bigbang
  - *deploy_bigbang
  # obtain initial images from full cluster state
  - ./scripts/list_images.sh > airgap/lists/images-initial.txt
  - cat airgap/lists/images-initial.txt
  # obtain true images from both lists
  - grep -v -x -f airgap/lists/images-false.txt airgap/lists/images-initial.txt > airgap/lists/images-ci.txt
  - cat airgap/lists/images-ci.txt
  # fail if the ci image list differs from the source image list
  - diff airgap/lists/images.txt airgap/lists/images-ci.txt
  # create images tarball
  # TODO - Change this to DOCKER_AUTH_CONFIG (possibly this, maybe something else) (Someone with admin needs to do this)
  - docker login registry1.dsop.io -u 'robot$bigbang' -p ${REGISTRY1_PASSWORD}
  - ./scripts/bundle_images.sh --image-list airgap/lists/images.txt --images airgap/tarballs/images.tar.gz

#-----------------------------------------------------------------------------------------------------------------------

#-----------------------------------------------------------------------------------------------------------------------
# Infrastructure: Management Jobs
#
# .pre job for pulling pipeline contents from umbrella-templates project
# TODO: Currently all jobs connected via "needs" must explicitly need this job, should evaluate turning this into a global cache

fetch umbrella templates:
  extends:
    - .fetch
  stage: .pre

# Abstract for job manually triggering infrastructure builds
.infra fork:
  stage: network up
  needs:
    - fetch umbrella templates
  rules:
    # Only run on merge requests when manually activated
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event"'
      when: manual
      allow_failure: false

# Abstract for jobs responsible for cleaning up infrastructure
.infra cleanup:
  rules:
    # Always run on merge requests, but allow failure
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event"'
      allow_failure: true

#-----------------------------------------------------------------------------------------------------------------------

#-----------------------------------------------------------------------------------------------------------------------
# Infrastructure: Networking
#

aws/network up:
  extends:
    - .infra fork
    - .network up
  environment:
    name: review/aws-${CI_COMMIT_REF_SLUG}-${CI_COMMIT_SHORT_SHA}
    auto_stop_in: 1 hour

aws/network down:
  extends:
    - .infra cleanup
    - .network down
  stage: network down
  environment:
    name: review/aws-${CI_COMMIT_REF_SLUG}-${CI_COMMIT_SHORT_SHA}
    action: stop
#-----------------------------------------------------------------------------------------------------------------------

#-----------------------------------------------------------------------------------------------------------------------
# Infrastructure: RKE2
#
# Create RKE2 cluster on AWS
aws/rke2/cluster up:
  stage: cluster up
  extends:
    - .rke2 up
  needs:
    - job: fetch umbrella templates
      artifacts: true
    - job: aws/network up

# Install BigBang on RKE2 cluster on AWS
aws/rke2/bigbang up:
  stage: bigbang up
  needs:
    - job: fetch umbrella templates
      artifacts: true
    - job: aws/rke2/cluster up
      artifacts: true
  before_script:
    - mkdir -p ~/.kube
    - cp ${CI_PROJECT_DIR}/rke2.yaml ~/.kube/config

    # Deploy a default storage class for aws
    - kubectl apply -f ${CI_PROJECT_DIR}/umbrella-templates/jobs/rke2/dependencies/k8s-resources/aws/default-ebs-sc.yaml
  script:
    - *deploy_bigbang

# Run tests on BigBang on RKE2 cluster on AWS
aws/rke2/bigbang test:
  stage: test
  needs:
    - job: fetch umbrella templates
      artifacts: true
    - job: aws/rke2/cluster up
      artifacts: true
    - job: aws/rke2/bigbang up
  before_script:
    - mkdir -p ~/.kube
    - cp ${CI_PROJECT_DIR}/rke2.yaml ~/.kube/config
  script:
    ## Move this yum install to the dockerfile for the builder
    ## putting it here now for a quick way to install dig
    - yum install bind-utils -y
    - ./scripts/hosts.sh
    - *test_bigbang

# Uninstall BigBang on RKE2 cluster on AWS
aws/rke2/bigbang down:
  stage: bigbang down
  extends:
    - .infra cleanup
  needs:
    - job: fetch umbrella templates
      artifacts: true
    - job: aws/rke2/cluster up
      artifacts: true
    - job: aws/rke2/bigbang test
  before_script:
    - mkdir -p ~/.kube
    - cp ${CI_PROJECT_DIR}/rke2.yaml ~/.kube/config
  script:
    - helm un -n bigbang bigbang
    # TODO: Smarter wait
    - sleep 180

# Destroy RKE2 cluster on AWS
aws/rke2/cluster down:
  stage: cluster down
  extends:
    - .infra cleanup
    - .rke2 down
  needs:
    - job: fetch umbrella templates
      artifacts: true
    - job: aws/rke2/bigbang down
#-----------------------------------------------------------------------------------------------------------------------